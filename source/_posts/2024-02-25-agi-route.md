---
layout: post
title: AGI的路线之争
date: 2024-02-25 23:28:09
categories: ai
cover: "./img/2024-02/170132856621530.jpg"
tags:
---
本文首发于matters，当前文章可见性：
| Blog  | Matters | Wechat | YuQue |
|------ |---------|--------|-------|
| [Link](https://david-zhang-test.github.io/2024/02/25/2024-02-25-agi-route/) | [Link](https://matters.town/@davidjohntest/529743-agi%E7%9A%84%E8%B7%AF%E7%BA%BF%E4%B9%8B%E4%BA%89-bafybeigyhmtla6dqzekjk7upuih76qkohreff42bts6hcfzacr5goyvwsu?utm_source=share_copy&referral=davidjohntest) | [Link](https://mp.weixin.qq.com/s?__biz=MzAwOTcwNjMzNQ==&mid=2247483946&idx=1&sn=268389a26bc0bf771f3f1e5c7f1591d2&chksm=9b5acc1aac2d450c61cb97f82c33452099bd107b13bca0d4cde5511fc6b496718d95bf643eec&token=787500227&lang=zh_CN#rd) | x |


> 智能，从哪里来？

OpenAI现在正是当红的小旦，资金、讨论、研究源源不断，openai背后的微软在这条道路的赌博中似乎已经占尽了优势。微软对头的google，从bard到gemini，也在逐渐向微软力大砖飞的线路合并。这种大力出奇迹的AGI研究道路似乎已经成为发展的基石，不再改变。


不过，前两日也看到Yann LeCun和他背后的meta公司的研究方向，从小数据量中掌握规律，再从这些规律中生成内容。
![](../img/2024-02/Screenshot%202024-02-25%20231140.png)
他们对小数据量的自信源于人类的基因，人的所有基因用二进制组合起来也不到8mb的大小，在如此小的数据量内就能表现出人类这样的智能。

相比之下，openai为代表的思路则是大力出奇迹，以巨大的数据量和超强的算力，直接让规律自然浮现，chatgpt和sora视频训练的数据量都是pb级别，10的9次方于meta的设想。

当然，在openai如日中天的当下，对meta路线选择的批评声是更多的。有人认为这个数据量被明显低估了，因为人类的基因不仅是一串二进制表达，还有空间形态等等信息隐含其中。
![](../img/2024-02/Screenshot%202024-02-25%20231919.png)
在我个人看来，openai和meta的根本分歧还在神经科学和脑科学当中。人类的意识研究和人脑研究还尚不完善，openai的训练方法对应人类通过大量的观察自发产生了智能，meta的训练方法对应人类通过少量规则的理解产生了智能。答案或许是两者其一，或是两种融合，抑或是跳脱两者之外。
